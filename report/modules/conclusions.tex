\section*{Conclusions}

The model giving us the best results comparing it to other Kaggle's notebooks is multinomial na\"ive Bayes with tfidf, we were in general surprised of how FastText performs worse than na\"ive Bayes; it seems to us that this model perform better when the dataset is simple as is the case of Sentiment140 while more complex have lower scores. 
Preprocessing also is not necessairly the best way, we obtained the best results when applying an ad-hoc preprocessing carefully choosing which words to ignore, a too broad preprocessing makes us lose too much information and have lower scores.