\section*{Conclusions}
One of the main conclusions that can be drawn from our experiments is that preprocessing must be done really carefully, we noticed that performing no preprocessing at all was better than performing spaCy's lemmatization and stopwords removal. 
It is interesting to note that also some of the most performing Kaggle's notebook improved their performance after removing some preprocessing from their pipeline (in particular the stopwords removal), this testifies how difficult is to get the preprocessing right.

The model giving us the best results is the multinomial na\"ive Bayes with tf-idf (without preprocessing or with just some ad-hoc stopwords removal), we were in general surprised of how using FastText performs worse than the classical approaches; the reason we gave to this phenomenon is that  more complex models than na\"ive Bayes, such as neural networks, are required in order to get the best out of word embeddings; also na\"ive Bayes works better with more classical and simple representations. 
Comparing our model to some models from Kaggle, we observed that, at least in this context, na\"ive Bayes can give competitive results even compared to models like LSTM, which is really surprising considered its simplicity.

%The model giving us the best results comparing it to other Kaggle's notebooks is multinomial na\"ive Bayes with tfidf, we were in general surprised of how FastText performs worse than na\"ive Bayes; it seems to us that this model perform better when the dataset is simple as is the case of Sentiment140 while more complex have lower scores. 
%Preprocessing also is not necessairly the best way, we obtained the best results when applying an ad-hoc preprocessing carefully choosing which words to ignore, a too broad preprocessing makes us lose too much information and have lower scores.